---
title: "HW4"
author: "Genavieve Middaugh"
date: "2025-02-14"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r global, echo= FALSE, warning = FALSE, message = FALSE}
library(tidyverse)
library(mosaic)
library(knitr)
library(kableExtra)


```


Git hub link 

[githubHW](https://github.com/crrankyypants/HW4)

## **Problem 1 - Iron Bank**

In this problem we are testing if the observed data is consistent with the Securities and Exchange Commissions, null hypothesis that over time security trades from Iron Bank are flagged at the same 2.4% rate as other traders.

**Null Hypothesis**: 2.4 will be our null hypothesis, this number comes from the Iron Bank employees trades that were flagged at this baseline rate of 0.024 compared to other traders.

**Test Statistic**: The number of flagged trades out of 2021 total trades which is 70/2021 = 0.03465

**Monte Carlo Simulation and P-Value**: Simulate 100000 trials where we assume each trade is 2.4% chance of being flagged. So after each trial, it will count how many trades get flagged. Below is the simulation.


```{r echo = FALSE, warning = FALSE, message = FALSE}

# given and usefull values
n_trades <- 2021
null_hypothesis <- 0.024
observed_flag <- 70 

# monte carlo sim
sim_results <- do(100000)* rbinom(1, n_trades, null_hypothesis)

p_value <- mean(sim_results$rbinom >= observed_flag)

ggplot(sim_results, aes(x = rbinom,y = ..density..)) + 
  geom_histogram(fill = "darkgreen", alpha = 0.7, bins = 25)+
  geom_vline(xintercept = observed_flag, color = "red", linetype = "dashed", size = 1) +
  labs(title = "Monte Carlo Simulated Distribution of Flagged Trades from SEC", x = "Number of Flagged Trades", y = "Probability Density")+
  theme_minimal()





```
The graph is bell shaped indicating little variation or skew, but there is a bit of inconsistency in the middle, with seemingly drastic leaps down and then upp again. The p-value we get is `r p_value`, so under the null hypothesis there is only a `r 100*(p_value)`% chance of observing 70 or more flagged trades by random chance. One would argue that this is relatively low number, it is below 0.05 and when you put it in context of the simulation, this means that the idea of being flagged 70 times is very odd, and doesn't or shouldn't happen often under a 2.4% flag rate. 

**Conclusion**: Since the p value is much smaller than 0.05 we have strong evidence to reject the null hypothesis. This value suggests that the flagged trade rate for Iron Bank employees is significantly higher than expected in the regular trading scene. 

\newpage
## **Problem 2 - Health Inspections**

The Health Department wants to ensure that any action taken is based on solid evidence that Gourmet Bites’
rate of health code violations is significantly higher than the citywide average of 3%.
In Problem 2 we are looking at a local health department that was investigating gourmet Bites. We know that Gourmet Bites was inspected 50 times out of the 1500 times that inspections were conducted, and we know that 8 of these 50 inspections led to citations. 

Question: Are the observed data for Gourmet Bites consistent with the Health Department’s null hypothesis
that, on average, restaurants in the city are cited for health code violations at the same 3% baseline rate?

**Hypothesis** null hypothesis is 0.03 restaurants in city are cited for health code violations at same 3% baseline rate

**Test Statistic** the number of health code violations in 50 inspections

Use a Monte Carlo simulation (with at least 100,000 simulations) to calculate a p-value under this null
hypothesis. 

```{r}
test_stat <- 50
null_hypothesis <- 0.03
observed <- 8

violations_sim <- do(100000) * rbinom(1, size = test_stat, prob = null_hypothesis)
df_sim <- data.frame(result = violations_sim$rbinom)

p_value <- mean(df_sim$result >= observed)

ggplot(df_sim, aes(x = result)) + geom_histogram(bins = 20, fill = "darkgreen") +
  geom_vline(xintercept = observed, color = "red", linetype = "dashed", size = 1.2) +
  labs(title = "Monte Carlo Simulation of resturants cited in a city for health \ncode violations. n = 50",
       subtitle = paste("Observed Violations =", observed, "| p-value =", round(p_value, 5)),
       x = "Number of Violations in 50 Inspections", y = "Frequency")



```

Our graph here is showing the Monte Carlo Simulation ran to simulate 50 of these Gourmet Bites restaurants being inspected and checking if they are cited at a 3% rate. When we compare our observed value of 8 citations out of the 50 inspections the p-value which is the mean of the results from this simulation as long as they are greater than our observed value of 8. That mean comes out to be `r round(p_value,5)`. This value is significantly lower than the basic standard of 0.05, this is suggesting that the 8 citations is out of the 3% range, and suggests that this is strong evidence against the null hypothesis that inspections resulting in citation is 3%.


## **Problem 3 - Evaluating Jury Selection for Bias**

In this problem we are using information known on how jury selection works and information given by the county which anonymized racial and ethnic categories into 5 groups.

breakdown of county eligible jury pool

Group 1  -  30 %
Group 2  -  25% 
Group 3  -  20% 
Group 4  -  15% 
Group 5  -  10%

Corresponding group counts for em-paneled Jurors in 20 Trials seen by judge (each jury has 12 jurors)

Group 1  - 85
Group 2  - 56 
Group 3  - 59 
Group 4  - 27 
Group 5  - 13

We want to determine if the distribution of jurors is significantly different from county population proportions. If so does this suggest systematic bias in jury selection? What other explanations might exist, and how could you investigate further?

Do determine this we will be using a Chi-Squared goodness of fit test to find if the distribution of em-paneled jurors is significantly deviant from the expected county proportions. 

**Null Hypothesis** this is the distribution of em-paneled jurors that match the expected distribution from the county jury pool. 

**Alt Hypothesis** The distribution of em-paneled jurors that do not match the expectations from the county jury pool.


```{r}
# Chi-Sqaured problem 
# hypothesis
n_jurors <- 12 
n_trials <- 20 
probs <- c(0.30, 0.25, 0.20, 0.15, 0.10)
observed <- c(85, 56, 59,27,13)
total_juror <- sum(observed)
expected <- total_juror * probs


chi_sq_test <- function(obs, exp){
  chi_squared_stat<-sum((obs - exp)^2 / exp)
  return(chi_squared_stat)
}

observed_chi <- chi_sq_test(observed, expected)


jury_sim <- do(100000)*{
  simulated_counts <- rmultinom(1, total_juror, probs)
  chi_sq_test(simulated_counts, expected)
}
df_chi <- data.frame(result = jury_sim$result)

p_value <- mean(jury_sim >= observed_chi)

ggplot(df_chi , aes(x = result)) + geom_histogram(binwidth = .5, fill = "darkgreen") +
  geom_vline(xintercept = observed_chi, color = "red", linetype = "dashed", size = 1.2) +
  labs(title = "Monte Carlo Simulation of Chi-Squared Statistics ", x = "Chi-Square Stat", y = "Frequency") 

```







## **Problem 4: LLM watermarking**

In problem 4 we are focusing on the watermarking of AI generated text.We are testing 10 sentences and testing if there is a watermark or not. 9 of them are normal sentences completely human made, and 1 was generated by a Large Language Model with a watermark. Which sentence is it?

```{r}
letter_frequencies <- read.csv("letter_frequencies.csv")

```

**Part A: the null or reference distribution**

Using data in the brown_sentences.txt file that contains collectionof english sentences fromt he Brown Corpus (well known and widely used text corpus in linguistics and natural language processing). Here we want to calculate a null distribution of hte chi-squared test statistic based on letter frequencies.

We want to know "what does the chi-squared statistic look like across lots of normal English sentences not generated by an LLM?"

```{r}

#1. Read the sentences: Load the sentences from brown_sentences.txt into your R environment. Look into the readLines function, which should be useful here (although not the only way).

lines <- readLines("brown_sentences.txt")


#2. Preprocess the text: For each sentence, remove non-letter characters, convert the text to uppercase,  and count the occurrences of each letter. (We did this in our Caesar cipher example; re-use that code as appropriate.)


calculate_chi_squared = function(sentence, freq_table) {
  
  # Remove non-letters and convert to uppercase
  clean_sentence = gsub("[^A-Za-z]", "", sentence)
  clean_sentence = toupper(clean_sentence)
  
  total_letters = nchar(clean_sentence)
  
  # check for zero values
  if (total_letters == 0){
    observed_df <- data.frame(Letter = LETTERS, Observed = rep(0,26))
    expected_df <- data.frame(letter = names(letter_frequencies), Expected = rep(0,26))
    chi_sq <- sum((observed_df$Observed - expected_df$Expected)^2 / (expected_df$Expected + 1e-9))
    return(chi_sq)
  }
  
  # Count the occurrences of each letter in the sentence
  observed_counts = table(factor(strsplit(clean_sentence, "")[[1]], levels = freq_table$Letter))
  observed_df <- as.data.frame(observed_counts)
  colnames(observed_df)<- c('Letter', 'Observed')
  #observed_counts <- observed_df$Observed[match(names(observed_counts), observed_df$Letter)]
  
  # Calculate expected counts
  expected_counts = total_letters * freq_table$Probability
  expected_df <- data.frame(Letter = freq_table$Letter, Expected = expected_counts)
  
  # Chi-squared statistic
  chi_squared_stat = sum((observed_counts - expected_counts)^2 / expected_counts)
  
  return(chi_squared_stat)
}

# collecting chisq stats from all sentences 


chi_squared <- sapply(lines, function(sentence){
  calculate_chi_squared(sentence, freq_table = letter_frequencies)
})
chi_squared <- chi_squared[!is.na(chi_squared)]

chi_summary <- data.frame(
  Min = min(chi_squared, na.rm = TRUE),
  Q1 = quantile(chi_squared, 0.25, na.rm = TRUE),
  Median = median(chi_squared, na.rm = TRUE),
  Mean = mean(chi_squared, na.rm = TRUE),
  Q3 = quantile(chi_squared, 0.75, na.rm = TRUE),
  Max = max(chi_squared, na.rm = TRUE)
)
kbl(chi_summary, caption = "Summary of chi Squared Statistics") %>% kable_styling(bootstrap_options = c("striped", "hover", "bordered"))


```

Above we are given the minimum chi-statistic is 4.6 ranging all the way to roughly 200. 

Then Below is the distribution that this summary comes from.

```{r}
chi_sq_df <- data.frame(Chi_Squared = chi_squared)

#6. Compile the distribution: Collect the chi-squared statistics from all sentences to form your reference or null distribution. This distribution represents the range of chi-squared values you might expect to see in normal English sentences based on the predefined letter frequency distribution.
ggplot(chi_sq_df, aes(x = Chi_Squared)) +
  geom_histogram(binwidth = 1, fill = "darkgreen")+
  labs(title = "Chi - Squared distribution of Sentence letter frequencies", x ="ChiSquared Statistics", y = "Frequency" )



```
This graph suggests that the differences between the observed counts of each letter in each sentence and the expected counts given the data are large. This then can be defined as the range of Chi-Squared values you could expect to see in normal English sentences based on the predefined letter frequency. We will use this for part B.

**Part B: checking for watermark**


```{r}
sentences <- c(
  "She opened the book and started to read the first chapter, eagerly anticipating what might come next.",
  "Despite the heavy rain, they decided to go for a long walk in the park, crossing the main avenue by the fountain in the center.",
  "The museum’s new exhibit features ancient artifacts from various civilizations around the world.",
  "He carefully examined the document, looking for any clues that might help solve the mystery.",
  "The students gathered in the auditorium to listen to the guest speaker’s inspiring lecture.",
  "Feeling vexed after an arduous and zany day at work, she hoped for a peaceful and quiet evening at home, cozying up after a quick dinner with some TV, or maybe a book on her upcoming visit to Auckland.",
  "The chef demonstrated how to prepare a delicious meal using only locally sourced ingredients, focusing mainly on some excellent dinner recipes from Spain.",
  "They watched the sunset from the hilltop, marveling at the beautiful array of colors in the sky.",
  "The committee reviewed the proposal and provided many points of useful feedback to improve the project’s effectiveness.",
  "Despite the challenges faced during the project, the team worked tirelessly to ensure its successful completion, resulting in a product that exceeded everyone’s expectations."
)

expected_distribution <- letter_frequencies$Probability / sum(letter_frequencies$Probability)
expected_distribution <- data.frame(
  Letter = tolower(letter_frequencies$Letter),  # Ensure lowercase letters
  Probability = letter_frequencies$Probability / sum(letter_frequencies$Probability)  # Normalize probabilities
)

letter_freq <- function(sentence){
  sentence <- tolower(sentence) 
  sentence <- gsub("[^a-z]", "", sentence) 
  total_letters = nchar(sentence)
  observed_counts <- table(factor(strsplit(sentence, "")[[1]], levels = letters))
  observed_df <- as.data.frame(observed_counts)
  colnames(observed_df)<- c('Letter', 'Observed')
  return(observed_df)
}

compute_chi <- function(sentence){
  observed_counts <- letter_freq(sentence)
  
  # Merge observed with expected distribution
  merged_data <- merge(observed_counts, expected_distribution, by = "Letter", all.x = TRUE)
  
  # Compute expected counts
  total_observed <- sum(merged_data$Observed)
  merged_data$Expected <- total_observed * merged_data$Probability
  
  # Chi-square test statistic
  chi_sq_stat <- sum((merged_data$Observed - merged_data$Expected)^2 / merged_data$Expected, na.rm = TRUE)
  
  # Compute p-value based on null distribution
  p_value <- mean(chi_sq_df >= chi_sq_stat)
  return(p_value)
}

pvalue <- sapply(sentences, compute_chi)

pvalue_df <- data.frame(Sentence = 1:10, P_Value =pvalue)

kbl(pvalue_df, caption = "Summary of P-Values from 10 Sentences", longtable = TRUE) %>% kable_styling(bootstrap_options = c("striped", "hover", "bordered"))


```

After Doing the calculations to get the P-value of each of these sentences based on hte Null distribution above, and the letter frequencies given, we look closely at these values and the lower the Pvalue the more unusual it is. Sentence number 6 is the AI generated sentence with a pvalue of 0.00878 this gives statistically signifigant evidence of sentence number 6 being the LLM sentence and not the English sentence.










